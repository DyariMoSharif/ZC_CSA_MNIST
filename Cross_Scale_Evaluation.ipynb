{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2631e48b-26c6-4054-bc2f-f3ae6cc87445",
   "metadata": {},
   "source": [
    "### MLP1L with different scaler (MLP1 generated Samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14609513-b1be-43a4-9df9-a120c8a495fd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results from Folder: ZC-CSA_images ===\n",
      " Index  True  Adversarial  Magnitude  ModelPred\n",
      "   102     2            7    1150.38          7\n",
      "   103     2            8    1133.78          2\n",
      "   104     2            7     852.70          7\n",
      "   107     2            7    1087.49          7\n",
      "   111     2            7    1155.91          7\n",
      "   114     2            7    1132.34          2\n",
      "   116     2            5    1152.05          2\n",
      "   118     2            7    1108.96          7\n",
      "    11     0            2    1138.52          0\n",
      "   120     2            7    1183.65          2\n",
      "   121     2            0    1138.58          0\n",
      "   123     2            8    1111.15          9\n",
      "   128     2            7    1124.17          2\n",
      "   139     2            3    1168.42          3\n",
      "    13     0            6    1126.73          6\n",
      "   141     2            6    1155.44          2\n",
      "   144     2            3     934.63          2\n",
      "   150     3            5    1166.40          3\n",
      "   153     3            5    1115.46          5\n",
      "   155     3            5    1165.49          5\n",
      "   158     3            5    1134.19          3\n",
      "   159     3            5    1185.14          3\n",
      "    15     0            2    1118.25          2\n",
      "   162     3            5    1147.49          5\n",
      "   163     3            5    1152.43          5\n",
      "   167     3            5    1125.65          3\n",
      "   168     3            2    1121.27          2\n",
      "   169     3            5    1149.45          3\n",
      "   171     3            8    1148.04          8\n",
      "   174     3            5    1190.89          5\n",
      "   176     3            5    1149.15          5\n",
      "   178     3            8    1138.91          8\n",
      "   179     3            5    1164.68          3\n",
      "   182     3            2    1156.65          3\n",
      "   185     3            2    1170.56          2\n",
      "   186     3            5    1144.06          5\n",
      "   192     3            2    1144.64          2\n",
      "   193     3            5    1155.70          5\n",
      "   194     3            5    1149.41          3\n",
      "     1     0            6     976.27          0\n",
      "   201     4            7    1162.73          4\n",
      "   202     4            7    1125.89          4\n",
      "   205     4            5    1136.09          4\n",
      "   206     4            7    1163.98          4\n",
      "   207     4            9    1175.66          9\n",
      "   208     4            7    1153.94          9\n",
      "   209     4            6    1122.29          4\n",
      "    20     0            2    1175.42          0\n",
      "   210     4            7    1150.17          4\n",
      "   211     4            2    1145.57          4\n",
      "   212     4            9    1155.34          9\n",
      "   213     4            6    1162.00          4\n",
      "   214     4            7    1132.09          9\n",
      "   215     4            2    1162.27          4\n",
      "   216     4            9    1170.82          4\n",
      "   217     4            5    1170.32          4\n",
      "    21     0            2    1145.01          2\n",
      "   220     4            7    1167.92          9\n",
      "   223     4            7    1130.14          4\n",
      "   224     4            8    1131.23          4\n",
      "   225     4            7    1157.92          7\n",
      "   226     4            7    1119.89          4\n",
      "   229     4            9    1166.41          4\n",
      "   230     4            9    1136.37          4\n",
      "   231     4            2    1162.65          4\n",
      "   232     4            7    1117.22          4\n",
      "   233     4            7    1192.69          4\n",
      "   234     4            2    1165.12          9\n",
      "   235     4            6    1173.18          6\n",
      "   236     4            5    1160.94          3\n",
      "   237     4            7    1138.88          4\n",
      "   238     4            7    1174.41          7\n",
      "   239     4            7    1123.75          7\n",
      "    23     0            2    1162.92          0\n",
      "   241     4            8    1142.41          9\n",
      "   242     4            7    1154.11          4\n",
      "   243     4            7    1148.52          4\n",
      "   244     4            7    1188.07          4\n",
      "   245     4            8    1165.13          4\n",
      "   246     4            9    1165.51          9\n",
      "   247     4            8    1125.75          9\n",
      "   248     4            2    1167.17          9\n",
      "   252     5            7    1147.16          1\n",
      "   253     5            7    1104.70          5\n",
      "   256     5            3    1168.67          9\n",
      "   258     5            2    1144.04          3\n",
      "   259     5            6    1143.67          8\n",
      "   260     5            3    1150.48          3\n",
      "   270     5            6    1132.26          6\n",
      "   275     5            6    1135.10          6\n",
      "   277     5            8    1131.08          8\n",
      "   281     5            3    1145.50          5\n",
      "   289     5            2    1193.98          0\n",
      "    28     0            7    1177.14          7\n",
      "   290     5            3    1124.30          5\n",
      "   291     5            3    1100.36          5\n",
      "   292     5            3    1111.12          5\n",
      "   293     5            3    1157.89          5\n",
      "   296     5            8    1143.71          5\n",
      "   297     5            8    1170.60          3\n",
      "   298     5            3    1087.68          5\n",
      "   299     5            6    1192.01          6\n",
      "   300     6            2    1160.63          6\n",
      "   301     6            2    1146.99          4\n",
      "   302     6            5    1210.13          8\n",
      "   303     6            0    1168.26          0\n",
      "   305     6            2    1157.49          6\n",
      "   306     6            2    1135.99          6\n",
      "   308     6            2    1181.27          6\n",
      "   313     6            2    1141.21          6\n",
      "   315     6            5    1164.16          6\n",
      "   316     6            5    1128.82          6\n",
      "   318     6            2    1163.08          6\n",
      "   319     6            2    1170.23          6\n",
      "   320     6            5    1173.41          5\n",
      "   323     6            2    1185.15          6\n",
      "   325     6            1    1146.30          6\n",
      "   326     6            2    1151.47          6\n",
      "   330     6            2    1155.90          2\n",
      "   333     6            0    1105.36          6\n",
      "   336     6            2    1170.17          6\n",
      "   337     6            0    1154.82          8\n",
      "   339     6            2    1152.74          6\n",
      "    33     0            2    1156.56          0\n",
      "   345     6            2    1148.59          6\n",
      "   346     6            7    1164.51          7\n",
      "   348     6            2    1148.06          6\n",
      "   350     7            5    1173.26          7\n",
      "   355     7            2    1162.93          2\n",
      "   356     7            2    1141.14          7\n",
      "   363     7            2    1172.50          2\n",
      "   365     7            2    1161.92          2\n",
      "    36     0            2    1180.60          0\n",
      "   370     7            2    1168.79          2\n",
      "   373     7            2    1126.99          2\n",
      "   374     7            2    1179.76          2\n",
      "   376     7            2    1160.42          2\n",
      "   378     7            2    1131.28          2\n",
      "    37     0            2    1149.64          2\n",
      "   380     7            2    1155.79          7\n",
      "   382     7            3    1148.60          7\n",
      "   389     7            5    1162.27          7\n",
      "    38     0            2    1172.94          2\n",
      "   390     7            2    1137.06          2\n",
      "   391     7            1    1179.68          7\n",
      "   392     7            2    1178.63          2\n",
      "   395     7            2    1153.24          2\n",
      "   396     7            2    1169.87          2\n",
      "     3     0            2    1172.68          0\n",
      "   400     8            5    1142.64          3\n",
      "   403     8            2    1167.28          8\n",
      "   404     8            5    1182.24          3\n",
      "   405     8            2    1165.11          7\n",
      "   406     8            5    1159.17          5\n",
      "   408     8            7    1168.58          8\n",
      "   409     8            9    1156.36          9\n",
      "    40     0            5    1159.55          0\n",
      "   410     8            2    1143.00          8\n",
      "   411     8            2    1189.78          8\n",
      "   412     8            3    1144.57          3\n",
      "   414     8            2    1138.69          3\n",
      "   415     8            5    1168.67          3\n",
      "   416     8            2    1127.65          2\n",
      "   417     8            3    1129.57          3\n",
      "   418     8            2    1164.63          2\n",
      "   419     8            2    1188.41          2\n",
      "    41     0            5    1177.04          0\n",
      "   420     8            5    1123.43          5\n",
      "   421     8            2    1147.26          8\n",
      "   423     8            3    1179.42          3\n",
      "   424     8            6    1140.94          6\n",
      "   425     8            3    1107.27          8\n",
      "   426     8            2    1163.29          8\n",
      "   427     8            2    1156.91          8\n",
      "   428     8            2    1147.20          2\n",
      "   430     8            2    1173.14          8\n",
      "   431     8            3    1171.69          8\n",
      "   433     8            2    1169.74          3\n",
      "   436     8            7    1180.17          8\n",
      "   438     8            2    1139.49          2\n",
      "   439     8            2    1109.50          8\n",
      "    43     0            2    1167.45          0\n",
      "   440     8            2    1138.61          3\n",
      "   441     8            2    1085.79          2\n",
      "   442     8            2    1155.72          8\n",
      "   443     8            2    1189.39          8\n",
      "   444     8            3    1152.65          8\n",
      "   445     8            2    1202.62          0\n",
      "   447     8            2    1181.89          8\n",
      "   448     8            2    1161.00          8\n",
      "   449     8            2    1121.40          2\n",
      "   450     9            7    1155.84          9\n",
      "   451     9            4    1135.22          9\n",
      "   452     9            7    1177.54          9\n",
      "   453     9            7    1155.67          7\n",
      "   454     9            3    1144.98          9\n",
      "   455     9            7    1203.38          9\n",
      "   456     9            7    1179.40          4\n",
      "   457     9            8    1177.60          9\n",
      "   458     9            5    1173.08          7\n",
      "   459     9            8    1157.64          8\n",
      "   460     9            5    1125.99          5\n",
      "   461     9            7    1166.63          9\n",
      "   462     9            7    1143.01          9\n",
      "   463     9            7    1141.03          9\n",
      "   464     9            0    1114.16          9\n",
      "   465     9            3    1220.33          9\n",
      "   466     9            7    1119.81          9\n",
      "   467     9            7    1112.88          9\n",
      "   468     9            7    1165.79          7\n",
      "   469     9            7    1136.68          9\n",
      "   470     9            7    1163.41          7\n",
      "   471     9            8    1154.87          9\n",
      "   472     9            5    1174.94          9\n",
      "   474     9            7    1121.94          9\n",
      "   475     9            7    1204.61          9\n",
      "   476     9            7    1178.97          7\n",
      "   477     9            7    1174.70          9\n",
      "   478     9            7    1142.58          7\n",
      "   479     9            7    1149.20          9\n",
      "   480     9            3    1146.35          3\n",
      "   481     9            7    1145.73          9\n",
      "   482     9            8    1154.24          9\n",
      "   483     9            2    1164.81          9\n",
      "   484     9            7    1108.08          7\n",
      "   485     9            8    1166.11          9\n",
      "   486     9            7    1126.27          9\n",
      "   487     9            7    1150.65          9\n",
      "   488     9            7    1135.62          9\n",
      "   489     9            7    1137.85          7\n",
      "   490     9            3    1161.21          7\n",
      "   491     9            7    1132.31          3\n",
      "   492     9            8    1155.54          8\n",
      "   493     9            2    1121.51          9\n",
      "   494     9            7    1160.17          7\n",
      "   495     9            7    1156.66          9\n",
      "   496     9            2    1168.47          1\n",
      "   497     9            5    1180.03          9\n",
      "   498     9            7    1198.83          7\n",
      "   499     9            7    1165.49          9\n",
      "    49     0            2    1135.87          0\n",
      "    50     1            2    1155.04          1\n",
      "    51     1            7    1172.88          1\n",
      "    52     1            2    1160.06          1\n",
      "    53     1            7    1208.24          1\n",
      "    54     1            2    1177.27          8\n",
      "    55     1            8    1176.34          8\n",
      "    56     1            2    1127.59          2\n",
      "    57     1            2    1159.94          1\n",
      "    58     1            3    1154.11          3\n",
      "    59     1            7    1201.36          7\n",
      "    60     1            2    1170.58          1\n",
      "    61     1            2    1181.28          2\n",
      "    62     1            7    1147.57          7\n",
      "    63     1            2    1176.07          2\n",
      "    64     1            2    1186.34          8\n",
      "    65     1            2    1163.00          2\n",
      "    66     1            2    1134.66          2\n",
      "    67     1            2    1187.51          1\n",
      "    68     1            2    1195.71          2\n",
      "    69     1            2    1155.91          2\n",
      "    70     1            8    1162.04          1\n",
      "    71     1            2    1166.64          2\n",
      "    72     1            2    1127.93          1\n",
      "    74     1            2    1154.73          2\n",
      "    75     1            2    1128.59          1\n",
      "    76     1            7    1137.71          1\n",
      "    79     1            2    1164.47          1\n",
      "     7     0            7    1192.85          0\n",
      "    80     1            5    1149.23          3\n",
      "    81     1            7    1124.09          1\n",
      "    82     1            2    1136.76          2\n",
      "    83     1            2    1148.79          1\n",
      "    84     1            2    1119.85          8\n",
      "    85     1            2    1121.29          1\n",
      "    86     1            2    1187.36          1\n",
      "    87     1            2    1117.62          1\n",
      "    88     1            7    1162.04          7\n",
      "    89     1            8    1167.26          1\n",
      "     8     0            2    1148.85          0\n",
      "    90     1            2    1156.10          1\n",
      "    91     1            2    1153.71          2\n",
      "    92     1            7    1167.65          7\n",
      "    93     1            7    1150.62          1\n",
      "    94     1            2    1148.85          8\n",
      "    95     1            2    1132.68          8\n",
      "    96     1            2    1174.57          1\n",
      "    97     1            7    1170.05          1\n",
      "    98     1            2    1179.12          8\n",
      "    99     1            7    1159.93          1\n",
      "\n",
      "✅ All formatted tables have been copied to the clipboard!\n",
      "\n",
      "=== Misclassification Summary by Folder ===\n",
      "ZC-CSA_images     136/ 290 misclassified   (46.90%)\n",
      "\n",
      "Overall misclassification rate:\n",
      "    136/290 samples   (46.90%)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pyperclip\n",
    "from pathlib import Path\n",
    "from io import StringIO\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model\n",
    "model = torch.jit.load(r'Models and Data splits\\MainModel_MLPSca.pt')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Root directory for your adversarial images\n",
    "adversarial_root = Path(r\"Generated Data\\ZC-CSA_images\\Adversarial\")\n",
    "all_image_paths = list(adversarial_root.rglob(\"*.png\"))\n",
    "\n",
    "# Step 1: Load everything into memory (vectors + metadata)\n",
    "vectors   = []\n",
    "metadata  = []\n",
    "\n",
    "for file_path in all_image_paths:\n",
    "    image = cv2.imread(str(file_path), cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        print(f\"Warning: Could not load image {file_path}\")\n",
    "        continue\n",
    "\n",
    "    # Parse context folder\n",
    "    context_folder = file_path.parts[-3] if len(file_path.parts) >= 3 else \"UnknownFolder\"\n",
    "\n",
    "    # Parse filename: e.g. '495_t9_p2_m814.39.png'\n",
    "    fname  = file_path.stem\n",
    "    parts  = fname.split(\"_\")\n",
    "    idx    = int(parts[0])\n",
    "    true_l = int(parts[1].lstrip(\"t\"))\n",
    "    pred_l = int(parts[2].lstrip(\"p\"))\n",
    "    mag    = float(parts[3].lstrip(\"m\"))\n",
    "\n",
    "    flat = image.astype(np.float32).flatten()\n",
    "    vectors.append(flat)\n",
    "    metadata.append({\n",
    "        \"folder\":   context_folder,\n",
    "        \"Index\":    idx,\n",
    "        \"True\":     true_l,\n",
    "        \"Adversarial\": pred_l,\n",
    "        \"Magnitude\":   mag\n",
    "    })\n",
    "\n",
    "if not vectors:\n",
    "    raise RuntimeError(\"No images found or loaded.\")\n",
    "\n",
    "# Step 2: Fit StandardScaler on the full dataset\n",
    "X = np.vstack(vectors)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 3: Convert to torch Tensor and run inference in one batch\n",
    "inputs = torch.tensor(X_scaled, dtype=torch.float32, device=device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "# Step 4: Aggregate results per folder\n",
    "results_by_folder = {}\n",
    "for meta, pred in zip(metadata, preds):\n",
    "    folder = meta.pop(\"folder\")\n",
    "    record = dict(meta, ModelPred=int(pred))\n",
    "    results_by_folder.setdefault(folder, []).append(record)\n",
    "\n",
    "# Step 5: Print tables and copy to clipboard\n",
    "output = StringIO()\n",
    "for folder, records in results_by_folder.items():\n",
    "    df = pd.DataFrame(records)\n",
    "    header    = f\"\\n=== Results from Folder: {folder} ===\\n\"\n",
    "    table_str = df.to_string(index=False)\n",
    "    print(header + table_str)\n",
    "    output.write(header)\n",
    "    output.write(table_str + \"\\n\")\n",
    "\n",
    "pyperclip.copy(output.getvalue())\n",
    "print(\"\\n✅ All formatted tables have been copied to the clipboard!\")\n",
    "\n",
    "\n",
    "# --- Misclassification Summary ---\n",
    "# Per‐folder\n",
    "print(\"\\n=== Misclassification Summary by Folder ===\")\n",
    "total_all = 0\n",
    "mismatch_all = 0\n",
    "\n",
    "for folder, records in results_by_folder.items():\n",
    "    df = pd.DataFrame(records)\n",
    "    total    = len(df)\n",
    "    mismatches = (df[\"True\"] != df[\"ModelPred\"]).sum()\n",
    "    pct      = mismatches / total * 100\n",
    "    print(f\"{folder:15s}  {mismatches:4d}/{total:4d} misclassified   ({pct:5.2f}%)\")\n",
    "    total_all    += total\n",
    "    mismatch_all += mismatches\n",
    "\n",
    "# Overall\n",
    "pct_all = mismatch_all / total_all * 100 if total_all else 0.0\n",
    "print(\"\\nOverall misclassification rate:\")\n",
    "print(f\"    {mismatch_all}/{total_all} samples   ({pct_all:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd3ce26-8db8-4a86-ac67-d7b7da150f05",
   "metadata": {},
   "source": [
    "### MLP1L with different scaler (CNN generated Samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32e2422a-c26b-46c0-af5d-dd6d5e38146a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results from Folder: cnn_ZC-CSA_images ===\n",
      " Index  True  Adversarial  Magnitude  ModelPred\n",
      "     0     0            8    1109.47          0\n",
      "   104     2            1    1157.97          2\n",
      "   106     2            8    1149.38          2\n",
      "   107     2            7    1160.00          2\n",
      "   117     2            7    1141.05          2\n",
      "   118     2            3    1166.82          3\n",
      "   119     2            8    1176.49          2\n",
      "   120     2            3    1159.97          2\n",
      "   121     2            3    1123.15          2\n",
      "   124     2            1    1099.96          2\n",
      "   129     2            1    1141.25          2\n",
      "   130     2            1    1154.87          2\n",
      "   135     2            7    1166.69          2\n",
      "   139     2            8    1157.05          2\n",
      "   141     2            3    1129.80          3\n",
      "   145     2            3    1149.30          2\n",
      "   154     3            9    1170.49          7\n",
      "   159     3            5    1127.94          3\n",
      "   160     3            9    1130.50          3\n",
      "   162     3            6    1142.89          3\n",
      "   167     3            5    1179.30          3\n",
      "   168     3            8    1172.90          3\n",
      "   182     3            9    1149.92          3\n",
      "    18     0            8    1121.18          0\n",
      "   198     3            5    1142.38          3\n",
      "   202     4            9    1127.53          3\n",
      "   204     4            8    1101.84          9\n",
      "   206     4            6    1160.73          2\n",
      "   209     4            9    1138.92          4\n",
      "   210     4            9    1125.79          4\n",
      "   211     4            7    1099.12          4\n",
      "   212     4            2    1132.07          4\n",
      "   216     4            7    1186.92          1\n",
      "   220     4            9    1079.66          4\n",
      "   224     4            8    1112.98          4\n",
      "   226     4            7    1186.55          5\n",
      "   227     4            7    1042.71          4\n",
      "   228     4            9    1178.40          4\n",
      "    22     0            5    1193.19          0\n",
      "   230     4            9    1159.77          4\n",
      "   235     4            9    1117.53          4\n",
      "    23     0            6    1156.09          0\n",
      "   241     4            9    1117.98          4\n",
      "   246     4            7    1149.19          7\n",
      "   251     5            8    1117.17          5\n",
      "   256     5            3    1099.76          5\n",
      "   263     5            9    1103.20          5\n",
      "   272     5            9    1127.11          9\n",
      "   277     5            6    1127.30          6\n",
      "    27     0            8    1087.77          0\n",
      "   282     5            9    1119.44          5\n",
      "   283     5            9    1145.11          5\n",
      "   287     5            6    1151.93          5\n",
      "   291     5            7    1153.31          1\n",
      "    29     0            5    1134.86          0\n",
      "     2     0            8    1110.99          0\n",
      "   303     6            5    1081.17          6\n",
      "    30     0            9    1136.59          0\n",
      "   310     6            8    1120.22          8\n",
      "   316     6            5    1142.93          6\n",
      "   322     6            2    1149.54          6\n",
      "   326     6            8    1120.20          6\n",
      "   333     6            5    1094.61          6\n",
      "   336     6            5    1125.95          6\n",
      "   339     6            0    1069.80          6\n",
      "    33     0            8    1158.03          0\n",
      "   358     7            3    1172.35          3\n",
      "   359     7            9    1141.49          7\n",
      "   366     7            9    1107.24          7\n",
      "    36     0            6    1128.56          0\n",
      "   375     7            9    1138.36          7\n",
      "   376     7            1    1150.80          7\n",
      "   377     7            2    1090.42          7\n",
      "   378     7            9    1122.04          7\n",
      "   379     7            1    1146.54          7\n",
      "   382     7            2    1102.37          7\n",
      "   387     7            2    1134.10          7\n",
      "   392     7            3    1101.57          7\n",
      "   397     7            3    1126.11          7\n",
      "   398     7            2    1104.91          2\n",
      "    39     0            9    1190.96          0\n",
      "   404     8            5    1131.49          8\n",
      "   406     8            7    1130.75          7\n",
      "   407     8            9    1143.65          8\n",
      "   408     8            2    1152.91          8\n",
      "   410     8            3    1164.01          3\n",
      "   411     8            2    1125.27          8\n",
      "   412     8            5    1159.46          8\n",
      "    41     0            9    1133.13          9\n",
      "   421     8            3    1152.12          8\n",
      "   423     8            5    1141.80          9\n",
      "    42     0            6    1166.80          0\n",
      "   430     8            2    1104.76          8\n",
      "   431     8            5    1156.90          8\n",
      "   432     8            5    1086.26          8\n",
      "   435     8            5    1156.72          8\n",
      "   438     8            6    1088.44          8\n",
      "   439     8            9    1123.40          9\n",
      "    43     0            2    1108.88          2\n",
      "   440     8            2    1130.09          8\n",
      "   443     8            2    1100.80          8\n",
      "   444     8            3    1107.42          3\n",
      "   445     8            2    1111.26          2\n",
      "   452     9            8    1148.96          9\n",
      "   454     9            7    1127.04          9\n",
      "   455     9            6    1118.68          9\n",
      "   457     9            5    1147.53          5\n",
      "   459     9            4    1169.84          4\n",
      "   465     9            7    1134.28          7\n",
      "   466     9            4    1138.85          9\n",
      "   467     9            5    1130.34          9\n",
      "   470     9            4    1121.81          5\n",
      "   474     9            7    1146.53          9\n",
      "   482     9            4    1141.53          9\n",
      "   483     9            4    1163.70          9\n",
      "   487     9            4    1111.57          9\n",
      "   490     9            4    1122.57          9\n",
      "   495     9            2    1176.67          9\n",
      "   499     9            4    1073.90          9\n",
      "    55     1            7    1139.06          7\n",
      "    65     1            4    1115.85          1\n",
      "     6     0            6    1134.46          0\n",
      "    86     1            8    1150.41          1\n",
      "\n",
      "✅ All formatted tables have been copied to the clipboard!\n",
      "\n",
      "=== Misclassification Summary by Folder ===\n",
      "cnn_ZC-CSA_images    28/ 123 misclassified   (22.76%)\n",
      "\n",
      "Overall misclassification rate:\n",
      "    28/123 samples   (22.76%)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pyperclip\n",
    "from pathlib import Path\n",
    "from io import StringIO\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model\n",
    "model = torch.jit.load(r'Models and Data splits\\MainModel_MLPSca.pt')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Root directory for your adversarial images\n",
    "adversarial_root = Path(r\"Generated Data\\cnn_ZC-CSA_images\\Adversarial\")\n",
    "all_image_paths = list(adversarial_root.rglob(\"*.png\"))\n",
    "\n",
    "# Step 1: Load everything into memory (vectors + metadata)\n",
    "vectors   = []\n",
    "metadata  = []\n",
    "\n",
    "for file_path in all_image_paths:\n",
    "    image = cv2.imread(str(file_path), cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        print(f\"Warning: Could not load image {file_path}\")\n",
    "        continue\n",
    "\n",
    "    # Parse context folder\n",
    "    context_folder = file_path.parts[-3] if len(file_path.parts) >= 3 else \"UnknownFolder\"\n",
    "\n",
    "    # Parse filename: e.g. '495_t9_p2_m814.39.png'\n",
    "    fname  = file_path.stem\n",
    "    parts  = fname.split(\"_\")\n",
    "    idx    = int(parts[0])\n",
    "    true_l = int(parts[1].lstrip(\"t\"))\n",
    "    pred_l = int(parts[2].lstrip(\"p\"))\n",
    "    mag    = float(parts[3].lstrip(\"m\"))\n",
    "\n",
    "    flat = image.astype(np.float32).flatten()\n",
    "    vectors.append(flat)\n",
    "    metadata.append({\n",
    "        \"folder\":   context_folder,\n",
    "        \"Index\":    idx,\n",
    "        \"True\":     true_l,\n",
    "        \"Adversarial\": pred_l,\n",
    "        \"Magnitude\":   mag\n",
    "    })\n",
    "\n",
    "if not vectors:\n",
    "    raise RuntimeError(\"No images found or loaded.\")\n",
    "\n",
    "# Step 2: Fit StandardScaler on the full dataset\n",
    "X = np.vstack(vectors)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 3: Convert to torch Tensor and run inference in one batch\n",
    "inputs = torch.tensor(X_scaled, dtype=torch.float32, device=device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "# Step 4: Aggregate results per folder\n",
    "results_by_folder = {}\n",
    "for meta, pred in zip(metadata, preds):\n",
    "    folder = meta.pop(\"folder\")\n",
    "    record = dict(meta, ModelPred=int(pred))\n",
    "    results_by_folder.setdefault(folder, []).append(record)\n",
    "\n",
    "# Step 5: Print tables and copy to clipboard\n",
    "output = StringIO()\n",
    "for folder, records in results_by_folder.items():\n",
    "    df = pd.DataFrame(records)\n",
    "    header    = f\"\\n=== Results from Folder: {folder} ===\\n\"\n",
    "    table_str = df.to_string(index=False)\n",
    "    print(header + table_str)\n",
    "    output.write(header)\n",
    "    output.write(table_str + \"\\n\")\n",
    "\n",
    "pyperclip.copy(output.getvalue())\n",
    "print(\"\\n✅ All formatted tables have been copied to the clipboard!\")\n",
    "\n",
    "\n",
    "# --- Misclassification Summary ---\n",
    "# Per‐folder\n",
    "print(\"\\n=== Misclassification Summary by Folder ===\")\n",
    "total_all = 0\n",
    "mismatch_all = 0\n",
    "\n",
    "for folder, records in results_by_folder.items():\n",
    "    df = pd.DataFrame(records)\n",
    "    total    = len(df)\n",
    "    mismatches = (df[\"True\"] != df[\"ModelPred\"]).sum()\n",
    "    pct      = mismatches / total * 100\n",
    "    print(f\"{folder:15s}  {mismatches:4d}/{total:4d} misclassified   ({pct:5.2f}%)\")\n",
    "    total_all    += total\n",
    "    mismatch_all += mismatches\n",
    "\n",
    "# Overall\n",
    "pct_all = mismatch_all / total_all * 100 if total_all else 0.0\n",
    "print(\"\\nOverall misclassification rate:\")\n",
    "print(f\"    {mismatch_all}/{total_all} samples   ({pct_all:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c0ea82-af0b-44c1-9350-66c7056fdf23",
   "metadata": {},
   "source": [
    "### CNN with different scaler (MLP1 generated Samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1921c4e9-d54a-4fe2-83cf-58f3519837ec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting StandardScaler on dataset …\n",
      "Scaler fitted.  μ ≈ 38.03, σ ≈ 66.71\n",
      "\n",
      "=== Results from Folder: ZC-CSA_images ===\n",
      " Index  True  Adversarial  Magnitude  ModelPred\n",
      "   102     2            7    1150.38          2\n",
      "   103     2            8    1133.78          8\n",
      "   104     2            7     852.70          2\n",
      "   107     2            7    1087.49          2\n",
      "   111     2            7    1155.91          2\n",
      "   114     2            7    1132.34          2\n",
      "   116     2            5    1152.05          2\n",
      "   118     2            7    1108.96          2\n",
      "    11     0            2    1138.52          0\n",
      "   120     2            7    1183.65          2\n",
      "   121     2            0    1138.58          2\n",
      "   123     2            8    1111.15          8\n",
      "   128     2            7    1124.17          2\n",
      "   139     2            3    1168.42          2\n",
      "    13     0            6    1126.73          0\n",
      "   141     2            6    1155.44          2\n",
      "   144     2            3     934.63          8\n",
      "   150     3            5    1166.40          3\n",
      "   153     3            5    1115.46          3\n",
      "   155     3            5    1165.49          3\n",
      "   158     3            5    1134.19          8\n",
      "   159     3            5    1185.14          3\n",
      "    15     0            2    1118.25          0\n",
      "   162     3            5    1147.49          8\n",
      "   163     3            5    1152.43          3\n",
      "   167     3            5    1125.65          3\n",
      "   168     3            2    1121.27          2\n",
      "   169     3            5    1149.45          3\n",
      "   171     3            8    1148.04          3\n",
      "   174     3            5    1190.89          3\n",
      "   176     3            5    1149.15          3\n",
      "   178     3            8    1138.91          3\n",
      "   179     3            5    1164.68          3\n",
      "   182     3            2    1156.65          3\n",
      "   185     3            2    1170.56          3\n",
      "   186     3            5    1144.06          8\n",
      "   192     3            2    1144.64          3\n",
      "   193     3            5    1155.70          8\n",
      "   194     3            5    1149.41          3\n",
      "     1     0            6     976.27          0\n",
      "   201     4            7    1162.73          4\n",
      "   202     4            7    1125.89          4\n",
      "   205     4            5    1136.09          4\n",
      "   206     4            7    1163.98          4\n",
      "   207     4            9    1175.66          4\n",
      "   208     4            7    1153.94          4\n",
      "   209     4            6    1122.29          4\n",
      "    20     0            2    1175.42          0\n",
      "   210     4            7    1150.17          4\n",
      "   211     4            2    1145.57          4\n",
      "   212     4            9    1155.34          8\n",
      "   213     4            6    1162.00          4\n",
      "   214     4            7    1132.09          4\n",
      "   215     4            2    1162.27          4\n",
      "   216     4            9    1170.82          4\n",
      "   217     4            5    1170.32          4\n",
      "    21     0            2    1145.01          0\n",
      "   220     4            7    1167.92          8\n",
      "   223     4            7    1130.14          4\n",
      "   224     4            8    1131.23          4\n",
      "   225     4            7    1157.92          4\n",
      "   226     4            7    1119.89          8\n",
      "   229     4            9    1166.41          4\n",
      "   230     4            9    1136.37          4\n",
      "   231     4            2    1162.65          4\n",
      "   232     4            7    1117.22          4\n",
      "   233     4            7    1192.69          4\n",
      "   234     4            2    1165.12          9\n",
      "   235     4            6    1173.18          4\n",
      "   236     4            5    1160.94          4\n",
      "   237     4            7    1138.88          4\n",
      "   238     4            7    1174.41          8\n",
      "   239     4            7    1123.75          4\n",
      "    23     0            2    1162.92          0\n",
      "   241     4            8    1142.41          4\n",
      "   242     4            7    1154.11          4\n",
      "   243     4            7    1148.52          4\n",
      "   244     4            7    1188.07          4\n",
      "   245     4            8    1165.13          4\n",
      "   246     4            9    1165.51          4\n",
      "   247     4            8    1125.75          8\n",
      "   248     4            2    1167.17          4\n",
      "   252     5            7    1147.16          8\n",
      "   253     5            7    1104.70          5\n",
      "   256     5            3    1168.67          8\n",
      "   258     5            2    1144.04          8\n",
      "   259     5            6    1143.67          5\n",
      "   260     5            3    1150.48          8\n",
      "   270     5            6    1132.26          5\n",
      "   275     5            6    1135.10          8\n",
      "   277     5            8    1131.08          5\n",
      "   281     5            3    1145.50          5\n",
      "   289     5            2    1193.98          8\n",
      "    28     0            7    1177.14          0\n",
      "   290     5            3    1124.30          5\n",
      "   291     5            3    1100.36          5\n",
      "   292     5            3    1111.12          5\n",
      "   293     5            3    1157.89          8\n",
      "   296     5            8    1143.71          5\n",
      "   297     5            8    1170.60          5\n",
      "   298     5            3    1087.68          5\n",
      "   299     5            6    1192.01          5\n",
      "   300     6            2    1160.63          6\n",
      "   301     6            2    1146.99          6\n",
      "   302     6            5    1210.13          6\n",
      "   303     6            0    1168.26          6\n",
      "   305     6            2    1157.49          6\n",
      "   306     6            2    1135.99          6\n",
      "   308     6            2    1181.27          6\n",
      "   313     6            2    1141.21          6\n",
      "   315     6            5    1164.16          6\n",
      "   316     6            5    1128.82          6\n",
      "   318     6            2    1163.08          6\n",
      "   319     6            2    1170.23          6\n",
      "   320     6            5    1173.41          6\n",
      "   323     6            2    1185.15          6\n",
      "   325     6            1    1146.30          6\n",
      "   326     6            2    1151.47          6\n",
      "   330     6            2    1155.90          6\n",
      "   333     6            0    1105.36          6\n",
      "   336     6            2    1170.17          6\n",
      "   337     6            0    1154.82          6\n",
      "   339     6            2    1152.74          2\n",
      "    33     0            2    1156.56          0\n",
      "   345     6            2    1148.59          6\n",
      "   346     6            7    1164.51          6\n",
      "   348     6            2    1148.06          6\n",
      "   350     7            5    1173.26          7\n",
      "   355     7            2    1162.93          8\n",
      "   356     7            2    1141.14          8\n",
      "   363     7            2    1172.50          2\n",
      "   365     7            2    1161.92          2\n",
      "    36     0            2    1180.60          0\n",
      "   370     7            2    1168.79          7\n",
      "   373     7            2    1126.99          7\n",
      "   374     7            2    1179.76          2\n",
      "   376     7            2    1160.42          2\n",
      "   378     7            2    1131.28          7\n",
      "    37     0            2    1149.64          0\n",
      "   380     7            2    1155.79          9\n",
      "   382     7            3    1148.60          7\n",
      "   389     7            5    1162.27          7\n",
      "    38     0            2    1172.94          2\n",
      "   390     7            2    1137.06          2\n",
      "   391     7            1    1179.68          7\n",
      "   392     7            2    1178.63          7\n",
      "   395     7            2    1153.24          7\n",
      "   396     7            2    1169.87          7\n",
      "     3     0            2    1172.68          0\n",
      "   400     8            5    1142.64          8\n",
      "   403     8            2    1167.28          8\n",
      "   404     8            5    1182.24          8\n",
      "   405     8            2    1165.11          8\n",
      "   406     8            5    1159.17          8\n",
      "   408     8            7    1168.58          8\n",
      "   409     8            9    1156.36          8\n",
      "    40     0            5    1159.55          0\n",
      "   410     8            2    1143.00          8\n",
      "   411     8            2    1189.78          8\n",
      "   412     8            3    1144.57          8\n",
      "   414     8            2    1138.69          8\n",
      "   415     8            5    1168.67          8\n",
      "   416     8            2    1127.65          8\n",
      "   417     8            3    1129.57          8\n",
      "   418     8            2    1164.63          8\n",
      "   419     8            2    1188.41          8\n",
      "    41     0            5    1177.04          0\n",
      "   420     8            5    1123.43          8\n",
      "   421     8            2    1147.26          8\n",
      "   423     8            3    1179.42          8\n",
      "   424     8            6    1140.94          8\n",
      "   425     8            3    1107.27          8\n",
      "   426     8            2    1163.29          8\n",
      "   427     8            2    1156.91          8\n",
      "   428     8            2    1147.20          8\n",
      "   430     8            2    1173.14          8\n",
      "   431     8            3    1171.69          8\n",
      "   433     8            2    1169.74          8\n",
      "   436     8            7    1180.17          8\n",
      "   438     8            2    1139.49          8\n",
      "   439     8            2    1109.50          8\n",
      "    43     0            2    1167.45          0\n",
      "   440     8            2    1138.61          8\n",
      "   441     8            2    1085.79          8\n",
      "   442     8            2    1155.72          8\n",
      "   443     8            2    1189.39          8\n",
      "   444     8            3    1152.65          8\n",
      "   445     8            2    1202.62          8\n",
      "   447     8            2    1181.89          8\n",
      "   448     8            2    1161.00          8\n",
      "   449     8            2    1121.40          8\n",
      "   450     9            7    1155.84          9\n",
      "   451     9            4    1135.22          9\n",
      "   452     9            7    1177.54          9\n",
      "   453     9            7    1155.67          9\n",
      "   454     9            3    1144.98          9\n",
      "   455     9            7    1203.38          9\n",
      "   456     9            7    1179.40          9\n",
      "   457     9            8    1177.60          9\n",
      "   458     9            5    1173.08          9\n",
      "   459     9            8    1157.64          8\n",
      "   460     9            5    1125.99          8\n",
      "   461     9            7    1166.63          8\n",
      "   462     9            7    1143.01          9\n",
      "   463     9            7    1141.03          9\n",
      "   464     9            0    1114.16          9\n",
      "   465     9            3    1220.33          9\n",
      "   466     9            7    1119.81          9\n",
      "   467     9            7    1112.88          9\n",
      "   468     9            7    1165.79          9\n",
      "   469     9            7    1136.68          9\n",
      "   470     9            7    1163.41          9\n",
      "   471     9            8    1154.87          9\n",
      "   472     9            5    1174.94          9\n",
      "   474     9            7    1121.94          9\n",
      "   475     9            7    1204.61          9\n",
      "   476     9            7    1178.97          9\n",
      "   477     9            7    1174.70          9\n",
      "   478     9            7    1142.58          9\n",
      "   479     9            7    1149.20          9\n",
      "   480     9            3    1146.35          9\n",
      "   481     9            7    1145.73          9\n",
      "   482     9            8    1154.24          9\n",
      "   483     9            2    1164.81          9\n",
      "   484     9            7    1108.08          8\n",
      "   485     9            8    1166.11          9\n",
      "   486     9            7    1126.27          9\n",
      "   487     9            7    1150.65          9\n",
      "   488     9            7    1135.62          9\n",
      "   489     9            7    1137.85          9\n",
      "   490     9            3    1161.21          8\n",
      "   491     9            7    1132.31          9\n",
      "   492     9            8    1155.54          9\n",
      "   493     9            2    1121.51          9\n",
      "   494     9            7    1160.17          9\n",
      "   495     9            7    1156.66          9\n",
      "   496     9            2    1168.47          8\n",
      "   497     9            5    1180.03          9\n",
      "   498     9            7    1198.83          9\n",
      "   499     9            7    1165.49          8\n",
      "    49     0            2    1135.87          9\n",
      "    50     1            2    1155.04          8\n",
      "    51     1            7    1172.88          8\n",
      "    52     1            2    1160.06          8\n",
      "    53     1            7    1208.24          8\n",
      "    54     1            2    1177.27          1\n",
      "    55     1            8    1176.34          8\n",
      "    56     1            2    1127.59          1\n",
      "    57     1            2    1159.94          1\n",
      "    58     1            3    1154.11          8\n",
      "    59     1            7    1201.36          1\n",
      "    60     1            2    1170.58          8\n",
      "    61     1            2    1181.28          8\n",
      "    62     1            7    1147.57          8\n",
      "    63     1            2    1176.07          8\n",
      "    64     1            2    1186.34          8\n",
      "    65     1            2    1163.00          2\n",
      "    66     1            2    1134.66          1\n",
      "    67     1            2    1187.51          8\n",
      "    68     1            2    1195.71          1\n",
      "    69     1            2    1155.91          7\n",
      "    70     1            8    1162.04          8\n",
      "    71     1            2    1166.64          8\n",
      "    72     1            2    1127.93          8\n",
      "    74     1            2    1154.73          1\n",
      "    75     1            2    1128.59          1\n",
      "    76     1            7    1137.71          8\n",
      "    79     1            2    1164.47          8\n",
      "     7     0            7    1192.85          6\n",
      "    80     1            5    1149.23          1\n",
      "    81     1            7    1124.09          8\n",
      "    82     1            2    1136.76          8\n",
      "    83     1            2    1148.79          1\n",
      "    84     1            2    1119.85          8\n",
      "    85     1            2    1121.29          1\n",
      "    86     1            2    1187.36          8\n",
      "    87     1            2    1117.62          1\n",
      "    88     1            7    1162.04          1\n",
      "    89     1            8    1167.26          8\n",
      "     8     0            2    1148.85          0\n",
      "    90     1            2    1156.10          8\n",
      "    91     1            2    1153.71          8\n",
      "    92     1            7    1167.65          8\n",
      "    93     1            7    1150.62          8\n",
      "    94     1            2    1148.85          8\n",
      "    95     1            2    1132.68          8\n",
      "    96     1            2    1174.57          8\n",
      "    97     1            7    1170.05          8\n",
      "    98     1            2    1179.12          8\n",
      "    99     1            7    1159.93          8\n",
      "\n",
      "✅ All formatted tables have been copied to the clipboard!\n",
      "\n",
      "=== Misclassification Summary by Folder ===\n",
      "ZC-CSA_images      74/ 290 misclassified   (25.52%)\n",
      "\n",
      "Overall misclassification rate:\n",
      "    74/290 samples   (25.52%)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pyperclip\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pathlib import Path\n",
    "from io import StringIO\n",
    "\n",
    "# 1. Setup device & load your saved CNN\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.jit.load(r\"Models and Data splits\\CNNScl.pt\").to(device)\n",
    "model.eval()\n",
    "\n",
    "# 2. Locate all adversarial PNGs\n",
    "adversarial_root = Path(r\"Generated Data\\ZC-CSA_images\\Adversarial\")\n",
    "all_image_paths  = list(adversarial_root.rglob(\"*.png\"))\n",
    "\n",
    "# 3. Fit a StandardScaler on *all* raw‑pixel values (0‑255)\n",
    "print(\"Fitting StandardScaler on dataset …\")\n",
    "pixel_bank = []\n",
    "for fp in all_image_paths:\n",
    "    img = cv2.imread(str(fp), cv2.IMREAD_GRAYSCALE)\n",
    "    if img is not None:\n",
    "        pixel_bank.append(img.astype(np.float32).reshape(-1, 1))  # keep 0‑255 range\n",
    "scaler = StandardScaler().fit(np.vstack(pixel_bank))\n",
    "mu    = scaler.mean_[0]            # or  scaler.mean_.item()\n",
    "sigma = np.sqrt(scaler.var_[0])    # or  np.sqrt(scaler.var_).item()\n",
    "print(f\"Scaler fitted.  μ ≈ {mu:.2f}, σ ≈ {sigma:.2f}\")\n",
    "\n",
    "\n",
    "results_by_folder = {}\n",
    "\n",
    "# 4. Inference loop\n",
    "for file_path in all_image_paths:\n",
    "    try:\n",
    "        # Load grayscale image\n",
    "        img = cv2.imread(str(file_path), cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            print(f\"Warning: Could not load image {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # --- StandardScaler normalisation & reshape to (1,1,H,W) ---\n",
    "        H, W      = img.shape\n",
    "        flat      = img.astype(np.float32).reshape(-1, 1)     # 0‑255 → float32\n",
    "        flat_norm = scaler.transform(flat).astype(np.float32) # standardise\n",
    "        img_norm  = flat_norm.reshape(H, W)\n",
    "\n",
    "        tensor = torch.from_numpy(img_norm).unsqueeze(0).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(tensor)\n",
    "        predicted = outputs.argmax(dim=1).item()\n",
    "\n",
    "        # Determine folder context\n",
    "        context_folder = file_path.parts[-3] if len(file_path.parts) >= 3 else \"UnknownFolder\"\n",
    "\n",
    "        # Parse filename: e.g. \"495_t9_p2_m814.39.png\"\n",
    "        fname = file_path.stem\n",
    "        idx, t_lbl, p_lbl, m_val = fname.split(\"_\")\n",
    "        index      = int(idx)\n",
    "        true_label = int(t_lbl.lstrip(\"t\"))\n",
    "        pred_label = int(p_lbl.lstrip(\"p\"))\n",
    "        magnitude  = float(m_val.lstrip(\"m\"))\n",
    "\n",
    "        # Accumulate results\n",
    "        results_by_folder.setdefault(context_folder, []).append({\n",
    "            \"Index\":       index,\n",
    "            \"True\":        true_label,\n",
    "            \"Adversarial\": pred_label,\n",
    "            \"Magnitude\":   magnitude,\n",
    "            \"ModelPred\":   predicted\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# 5. Print tables & copy to clipboard\n",
    "output = StringIO()\n",
    "for folder, records in results_by_folder.items():\n",
    "    df = pd.DataFrame(records)\n",
    "    header    = f\"\\n=== Results from Folder: {folder} ===\\n\"\n",
    "    table_str = df.to_string(index=False)\n",
    "    print(header + table_str)\n",
    "    output.write(header)\n",
    "    output.write(table_str + \"\\n\")\n",
    "\n",
    "pyperclip.copy(output.getvalue())\n",
    "print(\"\\n✅ All formatted tables have been copied to the clipboard!\")\n",
    "\n",
    "# --- Misclassification Summary ---\n",
    "print(\"\\n=== Misclassification Summary by Folder ===\")\n",
    "total_all     = 0\n",
    "mismatch_all  = 0\n",
    "\n",
    "for folder, records in results_by_folder.items():\n",
    "    df         = pd.DataFrame(records)\n",
    "    total      = len(df)\n",
    "    mismatches = (df[\"True\"] != df[\"ModelPred\"]).sum()\n",
    "    pct        = mismatches / total * 100\n",
    "    print(f\"{folder:15s}  {mismatches:4d}/{total:4d} misclassified   ({pct:5.2f}%)\")\n",
    "    total_all    += total\n",
    "    mismatch_all += mismatches\n",
    "\n",
    "pct_all = mismatch_all / total_all * 100 if total_all else 0.0\n",
    "print(\"\\nOverall misclassification rate:\")\n",
    "print(f\"    {mismatch_all}/{total_all} samples   ({pct_all:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579f94f0-ef2f-4f1f-9170-b570218561eb",
   "metadata": {},
   "source": [
    "### CNN with different scaler (CNN generated Samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bb75457-4ced-4a88-922c-0017b95cb4a6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting StandardScaler on dataset …\n",
      "Scaler fitted.  μ ≈ 39.83, σ ≈ 69.65\n",
      "\n",
      "=== Results from Folder: cnn_ZC-CSA_images ===\n",
      " Index  True  Adversarial  Magnitude  ModelPred\n",
      "     0     0            8    1109.47          8\n",
      "   104     2            1    1157.97          8\n",
      "   106     2            8    1149.38          2\n",
      "   107     2            7    1160.00          2\n",
      "   117     2            7    1141.05          2\n",
      "   118     2            3    1166.82          3\n",
      "   119     2            8    1176.49          8\n",
      "   120     2            3    1159.97          1\n",
      "   121     2            3    1123.15          2\n",
      "   124     2            1    1099.96          2\n",
      "   129     2            1    1141.25          2\n",
      "   130     2            1    1154.87          2\n",
      "   135     2            7    1166.69          2\n",
      "   139     2            8    1157.05          8\n",
      "   141     2            3    1129.80          3\n",
      "   145     2            3    1149.30          8\n",
      "   154     3            9    1170.49          8\n",
      "   159     3            5    1127.94          3\n",
      "   160     3            9    1130.50          3\n",
      "   162     3            6    1142.89          3\n",
      "   167     3            5    1179.30          3\n",
      "   168     3            8    1172.90          8\n",
      "   182     3            9    1149.92          8\n",
      "    18     0            8    1121.18          8\n",
      "   198     3            5    1142.38          3\n",
      "   202     4            9    1127.53          9\n",
      "   204     4            8    1101.84          8\n",
      "   206     4            6    1160.73          8\n",
      "   209     4            9    1138.92          9\n",
      "   210     4            9    1125.79          8\n",
      "   211     4            7    1099.12          4\n",
      "   212     4            2    1132.07          8\n",
      "   216     4            7    1186.92          8\n",
      "   220     4            9    1079.66          8\n",
      "   224     4            8    1112.98          8\n",
      "   226     4            7    1186.55          8\n",
      "   227     4            7    1042.71          4\n",
      "   228     4            9    1178.40          4\n",
      "    22     0            5    1193.19          0\n",
      "   230     4            9    1159.77          9\n",
      "   235     4            9    1117.53          8\n",
      "    23     0            6    1156.09          0\n",
      "   241     4            9    1117.98          4\n",
      "   246     4            7    1149.19          4\n",
      "   251     5            8    1117.17          8\n",
      "   256     5            3    1099.76          5\n",
      "   263     5            9    1103.20          5\n",
      "   272     5            9    1127.11          8\n",
      "   277     5            6    1127.30          6\n",
      "    27     0            8    1087.77          8\n",
      "   282     5            9    1119.44          8\n",
      "   283     5            9    1145.11          9\n",
      "   287     5            6    1151.93          5\n",
      "   291     5            7    1153.31          8\n",
      "    29     0            5    1134.86          5\n",
      "     2     0            8    1110.99          8\n",
      "   303     6            5    1081.17          6\n",
      "    30     0            9    1136.59          9\n",
      "   310     6            8    1120.22          8\n",
      "   316     6            5    1142.93          8\n",
      "   322     6            2    1149.54          8\n",
      "   326     6            8    1120.20          8\n",
      "   333     6            5    1094.61          6\n",
      "   336     6            5    1125.95          5\n",
      "   339     6            0    1069.80          6\n",
      "    33     0            8    1158.03          8\n",
      "   358     7            3    1172.35          7\n",
      "   359     7            9    1141.49          9\n",
      "   366     7            9    1107.24          8\n",
      "    36     0            6    1128.56          9\n",
      "   375     7            9    1138.36          9\n",
      "   376     7            1    1150.80          8\n",
      "   377     7            2    1090.42          2\n",
      "   378     7            9    1122.04          9\n",
      "   379     7            1    1146.54          8\n",
      "   382     7            2    1102.37          2\n",
      "   387     7            2    1134.10          8\n",
      "   392     7            3    1101.57          7\n",
      "   397     7            3    1126.11          7\n",
      "   398     7            2    1104.91          7\n",
      "    39     0            9    1190.96          0\n",
      "   404     8            5    1131.49          8\n",
      "   406     8            7    1130.75          8\n",
      "   407     8            9    1143.65          8\n",
      "   408     8            2    1152.91          2\n",
      "   410     8            3    1164.01          8\n",
      "   411     8            2    1125.27          8\n",
      "   412     8            5    1159.46          8\n",
      "    41     0            9    1133.13          9\n",
      "   421     8            3    1152.12          8\n",
      "   423     8            5    1141.80          8\n",
      "    42     0            6    1166.80          6\n",
      "   430     8            2    1104.76          8\n",
      "   431     8            5    1156.90          8\n",
      "   432     8            5    1086.26          8\n",
      "   435     8            5    1156.72          8\n",
      "   438     8            6    1088.44          8\n",
      "   439     8            9    1123.40          8\n",
      "    43     0            2    1108.88          2\n",
      "   440     8            2    1130.09          8\n",
      "   443     8            2    1100.80          8\n",
      "   444     8            3    1107.42          8\n",
      "   445     8            2    1111.26          8\n",
      "   452     9            8    1148.96          8\n",
      "   454     9            7    1127.04          9\n",
      "   455     9            6    1118.68          9\n",
      "   457     9            5    1147.53          9\n",
      "   459     9            4    1169.84          9\n",
      "   465     9            7    1134.28          8\n",
      "   466     9            4    1138.85          9\n",
      "   467     9            5    1130.34          9\n",
      "   470     9            4    1121.81          8\n",
      "   474     9            7    1146.53          8\n",
      "   482     9            4    1141.53          9\n",
      "   483     9            4    1163.70          9\n",
      "   487     9            4    1111.57          9\n",
      "   490     9            4    1122.57          9\n",
      "   495     9            2    1176.67          9\n",
      "   499     9            4    1073.90          8\n",
      "    55     1            7    1139.06          8\n",
      "    65     1            4    1115.85          8\n",
      "     6     0            6    1134.46          8\n",
      "    86     1            8    1150.41          8\n",
      "\n",
      "✅ All formatted tables have been copied to the clipboard!\n",
      "\n",
      "=== Misclassification Summary by Folder ===\n",
      "cnn_ZC-CSA_images    63/ 123 misclassified   (51.22%)\n",
      "\n",
      "Overall misclassification rate:\n",
      "    63/123 samples   (51.22%)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pyperclip\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pathlib import Path\n",
    "from io import StringIO\n",
    "\n",
    "# 1. Setup device & load your saved CNN\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.jit.load(r\"Models and Data splits\\CNNScl.pt\").to(device)\n",
    "model.eval()\n",
    "\n",
    "# 2. Locate all adversarial PNGs\n",
    "adversarial_root = Path(r\"Generated Data\\cnn_ZC-CSA_images\\Adversarial\")\n",
    "all_image_paths  = list(adversarial_root.rglob(\"*.png\"))\n",
    "\n",
    "# 3. Fit a StandardScaler on *all* raw‑pixel values (0‑255)\n",
    "print(\"Fitting StandardScaler on dataset …\")\n",
    "pixel_bank = []\n",
    "for fp in all_image_paths:\n",
    "    img = cv2.imread(str(fp), cv2.IMREAD_GRAYSCALE)\n",
    "    if img is not None:\n",
    "        pixel_bank.append(img.astype(np.float32).reshape(-1, 1))  # keep 0‑255 range\n",
    "scaler = StandardScaler().fit(np.vstack(pixel_bank))\n",
    "mu    = scaler.mean_[0]            # or  scaler.mean_.item()\n",
    "sigma = np.sqrt(scaler.var_[0])    # or  np.sqrt(scaler.var_).item()\n",
    "print(f\"Scaler fitted.  μ ≈ {mu:.2f}, σ ≈ {sigma:.2f}\")\n",
    "\n",
    "\n",
    "results_by_folder = {}\n",
    "\n",
    "# 4. Inference loop\n",
    "for file_path in all_image_paths:\n",
    "    try:\n",
    "        # Load grayscale image\n",
    "        img = cv2.imread(str(file_path), cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            print(f\"Warning: Could not load image {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # --- StandardScaler normalisation & reshape to (1,1,H,W) ---\n",
    "        H, W      = img.shape\n",
    "        flat      = img.astype(np.float32).reshape(-1, 1)     # 0‑255 → float32\n",
    "        flat_norm = scaler.transform(flat).astype(np.float32) # standardise\n",
    "        img_norm  = flat_norm.reshape(H, W)\n",
    "\n",
    "        tensor = torch.from_numpy(img_norm).unsqueeze(0).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(tensor)\n",
    "        predicted = outputs.argmax(dim=1).item()\n",
    "\n",
    "        # Determine folder context\n",
    "        context_folder = file_path.parts[-3] if len(file_path.parts) >= 3 else \"UnknownFolder\"\n",
    "\n",
    "        # Parse filename: e.g. \"495_t9_p2_m814.39.png\"\n",
    "        fname = file_path.stem\n",
    "        idx, t_lbl, p_lbl, m_val = fname.split(\"_\")\n",
    "        index      = int(idx)\n",
    "        true_label = int(t_lbl.lstrip(\"t\"))\n",
    "        pred_label = int(p_lbl.lstrip(\"p\"))\n",
    "        magnitude  = float(m_val.lstrip(\"m\"))\n",
    "\n",
    "        # Accumulate results\n",
    "        results_by_folder.setdefault(context_folder, []).append({\n",
    "            \"Index\":       index,\n",
    "            \"True\":        true_label,\n",
    "            \"Adversarial\": pred_label,\n",
    "            \"Magnitude\":   magnitude,\n",
    "            \"ModelPred\":   predicted\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# 5. Print tables & copy to clipboard\n",
    "output = StringIO()\n",
    "for folder, records in results_by_folder.items():\n",
    "    df = pd.DataFrame(records)\n",
    "    header    = f\"\\n=== Results from Folder: {folder} ===\\n\"\n",
    "    table_str = df.to_string(index=False)\n",
    "    print(header + table_str)\n",
    "    output.write(header)\n",
    "    output.write(table_str + \"\\n\")\n",
    "\n",
    "pyperclip.copy(output.getvalue())\n",
    "print(\"\\n✅ All formatted tables have been copied to the clipboard!\")\n",
    "\n",
    "# --- Misclassification Summary ---\n",
    "print(\"\\n=== Misclassification Summary by Folder ===\")\n",
    "total_all     = 0\n",
    "mismatch_all  = 0\n",
    "\n",
    "for folder, records in results_by_folder.items():\n",
    "    df         = pd.DataFrame(records)\n",
    "    total      = len(df)\n",
    "    mismatches = (df[\"True\"] != df[\"ModelPred\"]).sum()\n",
    "    pct        = mismatches / total * 100\n",
    "    print(f\"{folder:15s}  {mismatches:4d}/{total:4d} misclassified   ({pct:5.2f}%)\")\n",
    "    total_all    += total\n",
    "    mismatch_all += mismatches\n",
    "\n",
    "pct_all = mismatch_all / total_all * 100 if total_all else 0.0\n",
    "print(\"\\nOverall misclassification rate:\")\n",
    "print(f\"    {mismatch_all}/{total_all} samples   ({pct_all:.2f}%)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:GPUEnabled]",
   "language": "python",
   "name": "conda-env-GPUEnabled-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
