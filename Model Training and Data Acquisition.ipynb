{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a166aa89-649e-497f-8548-d57e599873d3",
   "metadata": {},
   "source": [
    "### Display the versions of the libraries used for reference purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27a17ea8-ff02-4ec4-a52f-52170456ea71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:49:16) [MSC v.1929 64 bit (AMD64)]\n",
      "Jupyter Notebook version: 7.3.2\n",
      "NumPy version: 2.0.1\n",
      "TensorFlow version: 2.19.0\n",
      "Torch version: 2.6.0+cu126\n",
      "Scikit-learn version: 1.6.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import torch\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import notebook\n",
    "\n",
    "# Print Python version\n",
    "print(f'Python version: {sys.version}')\n",
    "\n",
    "# Print Jupyter Notebook version\n",
    "print(f'Jupyter Notebook version: {notebook.__version__}')\n",
    "\n",
    "# Print library versions\n",
    "print(f'NumPy version: {np.__version__}')\n",
    "print(f'TensorFlow version: {tf.__version__}')\n",
    "print(f'Torch version: {torch.__version__}')\n",
    "print(f'Scikit-learn version: {sklearn.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbc61aa-b6bc-4112-85c2-8e2bf483bd8b",
   "metadata": {},
   "source": [
    "### Loading MNIST dataset and Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d18becb2-7b4e-4570-b486-65b4c0b232f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Models and Data splits/[SCALED] Train_Test_Splits.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "import joblib\n",
    "\n",
    "# -------------------------\n",
    "# Load and Preprocess the MNIST Data\n",
    "# -------------------------\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X = mnist.data.values\n",
    "y = mnist.target.astype(int).values\n",
    "\n",
    "# Split the data into training and testing sets (using random_state=42)\n",
    "X_train_not_scaled, X_test_not_scaled, y_train_not_scaled, y_test_not_scaled = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "\n",
    "# Save the numpy arrays for the train and test splits BEFORE scaling\n",
    "joblib.dump((X_train_not_scaled, X_test_not_scaled, y_train_not_scaled, y_test_not_scaled), 'Models and Data splits/[ORIGINAL] Train_Test_Splits.pkl')\n",
    "\n",
    "X_train_scaled  = X_train_not_scaled /255.0\n",
    "X_test_scaled  = X_test_not_scaled  /255.0\n",
    "y_train_scaled  = y_train_not_scaled\n",
    "y_test_scaled  = y_test_not_scaled \n",
    "\n",
    "# Save the numpy arrays for the train and test splits AFTER scaling\n",
    "joblib.dump((X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled), 'Models and Data splits/[SCALED] Train_Test_Splits.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f43dbba-da1b-409c-8b28-5fa4fd0788a6",
   "metadata": {},
   "source": [
    "### Training the main model MLP with one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ef2ee84-3869-431a-9be5-6ed08c3b7669",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.3059\n",
      "Epoch [2/100], Loss: 0.1286\n",
      "Epoch [3/100], Loss: 0.0864\n",
      "Epoch [4/100], Loss: 0.0638\n",
      "Epoch [5/100], Loss: 0.0478\n",
      "Epoch [6/100], Loss: 0.0360\n",
      "Epoch [7/100], Loss: 0.0270\n",
      "Epoch [8/100], Loss: 0.0210\n",
      "Epoch [9/100], Loss: 0.0163\n",
      "Epoch [10/100], Loss: 0.0139\n",
      "Epoch [11/100], Loss: 0.0110\n",
      "Epoch [12/100], Loss: 0.0088\n",
      "Epoch [13/100], Loss: 0.0068\n",
      "Epoch [14/100], Loss: 0.0076\n",
      "Epoch [15/100], Loss: 0.0053\n",
      "Epoch [16/100], Loss: 0.0056\n",
      "Epoch [17/100], Loss: 0.0053\n",
      "Epoch [18/100], Loss: 0.0045\n",
      "Epoch [19/100], Loss: 0.0039\n",
      "Epoch [20/100], Loss: 0.0033\n",
      "Epoch [21/100], Loss: 0.0047\n",
      "Epoch [22/100], Loss: 0.0025\n",
      "Epoch [23/100], Loss: 0.0054\n",
      "Epoch [24/100], Loss: 0.0027\n",
      "Epoch [25/100], Loss: 0.0049\n",
      "Epoch [26/100], Loss: 0.0034\n",
      "Epoch [27/100], Loss: 0.0039\n",
      "Epoch [28/100], Loss: 0.0027\n",
      "Epoch [29/100], Loss: 0.0042\n",
      "Epoch [30/100], Loss: 0.0023\n",
      "Epoch [31/100], Loss: 0.0008\n",
      "Epoch [32/100], Loss: 0.0002\n",
      "Epoch [33/100], Loss: 0.0001\n",
      "Epoch [34/100], Loss: 0.0000\n",
      "Epoch [35/100], Loss: 0.0000\n",
      "Epoch [36/100], Loss: 0.0000\n",
      "Epoch [37/100], Loss: 0.0000\n",
      "Epoch [38/100], Loss: 0.0090\n",
      "Epoch [39/100], Loss: 0.0061\n",
      "Epoch [40/100], Loss: 0.0014\n",
      "Epoch [41/100], Loss: 0.0017\n",
      "Epoch [42/100], Loss: 0.0061\n",
      "Epoch [43/100], Loss: 0.0010\n",
      "Epoch [44/100], Loss: 0.0009\n",
      "Epoch [45/100], Loss: 0.0037\n",
      "Epoch [46/100], Loss: 0.0047\n",
      "Epoch [47/100], Loss: 0.0010\n",
      "Epoch [48/100], Loss: 0.0002\n",
      "Epoch [49/100], Loss: 0.0000\n",
      "Epoch [50/100], Loss: 0.0000\n",
      "Epoch [51/100], Loss: 0.0000\n",
      "Epoch [52/100], Loss: 0.0000\n",
      "Epoch [53/100], Loss: 0.0000\n",
      "Epoch [54/100], Loss: 0.0000\n",
      "Epoch [55/100], Loss: 0.0000\n",
      "Epoch [56/100], Loss: 0.0000\n",
      "Epoch [57/100], Loss: 0.0000\n",
      "Epoch [58/100], Loss: 0.0147\n",
      "Epoch [59/100], Loss: 0.0020\n",
      "Epoch [60/100], Loss: 0.0008\n",
      "Epoch [61/100], Loss: 0.0029\n",
      "Epoch [62/100], Loss: 0.0016\n",
      "Epoch [63/100], Loss: 0.0002\n",
      "Epoch [64/100], Loss: 0.0057\n",
      "Epoch [65/100], Loss: 0.0021\n",
      "Epoch [66/100], Loss: 0.0003\n",
      "Epoch [67/100], Loss: 0.0019\n",
      "Epoch [68/100], Loss: 0.0035\n",
      "Epoch [69/100], Loss: 0.0027\n",
      "Epoch [70/100], Loss: 0.0008\n",
      "Epoch [71/100], Loss: 0.0003\n",
      "Epoch [72/100], Loss: 0.0046\n",
      "Epoch [73/100], Loss: 0.0016\n",
      "Epoch [74/100], Loss: 0.0008\n",
      "Epoch [75/100], Loss: 0.0003\n",
      "Epoch [76/100], Loss: 0.0010\n",
      "Epoch [77/100], Loss: 0.0072\n",
      "Epoch [78/100], Loss: 0.0017\n",
      "Epoch [79/100], Loss: 0.0002\n",
      "Epoch [80/100], Loss: 0.0000\n",
      "Epoch [81/100], Loss: 0.0000\n",
      "Epoch [82/100], Loss: 0.0000\n",
      "Epoch [83/100], Loss: 0.0000\n",
      "Epoch [84/100], Loss: 0.0000\n",
      "Epoch [85/100], Loss: 0.0000\n",
      "Epoch [86/100], Loss: 0.0000\n",
      "Epoch [87/100], Loss: 0.0000\n",
      "Epoch [88/100], Loss: 0.0000\n",
      "Epoch [89/100], Loss: 0.0000\n",
      "Epoch [90/100], Loss: 0.0000\n",
      "Epoch [91/100], Loss: 0.0000\n",
      "Epoch [92/100], Loss: 0.0000\n",
      "Epoch [93/100], Loss: 0.0000\n",
      "Epoch [94/100], Loss: 0.0000\n",
      "Epoch [95/100], Loss: 0.0123\n",
      "Epoch [96/100], Loss: 0.0008\n",
      "Epoch [97/100], Loss: 0.0005\n",
      "Epoch [98/100], Loss: 0.0029\n",
      "Epoch [99/100], Loss: 0.0021\n",
      "Epoch [100/100], Loss: 0.0030\n",
      "\n",
      "Accuracy: 0.9809\n",
      "Precision: 0.9808\n",
      "Recall: 0.9808\n",
      "F1 Score: 0.9808\n",
      "--------------\n",
      "Confusion Matrix:\n",
      " [[1368    1    1    1    0    2    0    2    2    4]\n",
      " [   0 1562    5    1    1    0    0    5    1    0]\n",
      " [   4    4 1364    6    2    2    2    5    7    2]\n",
      " [   2    2    9 1391    0   16    0    3    5    0]\n",
      " [   3    2    2    0 1337    0    7    2    0   12]\n",
      " [   2    1    0    3    2 1236    5    0   10    4]\n",
      " [   4    3    0    1    2    4 1360    0    1    0]\n",
      " [   1    3   13    1    5    0    0 1428    2    6]\n",
      " [   7    2    3    7    1    5    6    1 1330    3]\n",
      " [   0    2    0    5    7    5    0   11    4 1357]]\n",
      "False Alarms for each class: [23 20 33 25 20 34 20 29 32 31]\n",
      "False Alarms for all classes: 267\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import random\n",
    "import joblib\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# -------------------------\n",
    "# Set seeds for reproducibility\n",
    "# -------------------------\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    # Optional: for determinism (may impact performance)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# Loading the scaled MNIST Data\n",
    "# -------------------------\n",
    "\n",
    "X_train_np, X_test_np, y_train_np, y_test_np = joblib.load( 'Models and Data splits/[SCALED] Train_Test_Splits.pkl')\n",
    "\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train = torch.tensor(X_train_np, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test_np, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_np, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test_np, dtype=torch.long)\n",
    "\n",
    "# Create DataLoaders for mini-batch training\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# -------------------------\n",
    "# Define the MLP Model using PyTorch\n",
    "# -------------------------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_units, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_units)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_units, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        # The output logits will be used in the CrossEntropyLoss, so no softmax here\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_units=256, output_dim=10)\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# -------------------------\n",
    "# Train the Model\n",
    "# -------------------------\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # Move data to GPU if available\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Evaluate the Model on the Test Data\n",
    "# -------------------------\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_true = []\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        # Get predicted class using the index with maximum logit score\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.append(predicted.cpu().numpy())\n",
    "        all_true.append(batch_y.cpu().numpy())\n",
    "\n",
    "y_pred = np.concatenate(all_preds)\n",
    "y_true = np.concatenate(all_true)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='macro')\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"--------------\")\n",
    "\n",
    "# Compute the confusion matrix and false positives per class\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "false_alarms = cm.sum(axis=0) - np.diag(cm)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"False Alarms for each class:\", false_alarms)\n",
    "print(\"False Alarms for all classes:\", false_alarms.sum())\n",
    "\n",
    "\n",
    "\n",
    "# Save the scripted model\n",
    "model = torch.jit.script(model)\n",
    "torch.jit.save(model, 'Models and Data splits/MainModel_MLP1L.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f706e481-b0b2-4a9a-8169-11cb8c968d7f",
   "metadata": {},
   "source": [
    "## Loading and using the same train-test split for other methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bf1535-2e46-4d89-9e1f-c764306a765e",
   "metadata": {},
   "source": [
    "### MLP with 2 layers, each having 256 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c118f87-c4f8-4900-a453-65d435396863",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/100], Loss: 0.2693\n",
      "Epoch [10/100], Loss: 0.0172\n",
      "Epoch [20/100], Loss: 0.0123\n",
      "Epoch [30/100], Loss: 0.0065\n",
      "Epoch [40/100], Loss: 0.0054\n",
      "Epoch [50/100], Loss: 0.0023\n",
      "Epoch [60/100], Loss: 0.0034\n",
      "Epoch [70/100], Loss: 0.0022\n",
      "Epoch [80/100], Loss: 0.0049\n",
      "Epoch [90/100], Loss: 0.0047\n",
      "Epoch [100/100], Loss: 0.0000\n",
      "\n",
      "Evaluation Metrics:\n",
      "Accuracy: 0.9836\n",
      "Precision: 0.9836\n",
      "Recall: 0.9835\n",
      "F1 Score: 0.9835\n",
      "------------------------\n",
      "Confusion Matrix:\n",
      " [[1377    1    1    0    0    0    1    0    1    0]\n",
      " [   1 1557    5    5    1    0    1    4    1    0]\n",
      " [   3    2 1377    2    0    1    1    7    4    1]\n",
      " [   0    0    5 1401    0   10    0    3    8    1]\n",
      " [   1    4    2    0 1334    0    5    0    0   19]\n",
      " [   2    0    3    3    1 1228   10    2    8    6]\n",
      " [   2    2    0    1    3    5 1361    0    1    0]\n",
      " [   2    3    6    1   10    0    1 1433    2    1]\n",
      " [   2    4    2    2    1    1    4    1 1341    7]\n",
      " [   2    2    0    5    9    2    1    5    3 1362]]\n",
      "False Alarms for each class: [15 18 24 19 25 19 24 22 28 35]\n",
      "False Alarms for all classes: 229\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------\n",
    "# Set seeds for reproducibility\n",
    "# -------------------------\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    # Optional: for determinism (may impact performance)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -------------------------\n",
    "# Loading the scaled MNIST Data\n",
    "# -------------------------\n",
    "X_train_np, X_test_np, y_train_np, y_test_np = joblib.load( 'Models and Data splits/[SCALED] Train_Test_Splits.pkl')\n",
    "\n",
    "\n",
    "# Convert the numpy arrays to torch tensors and send them to the GPU (if available)\n",
    "# Ensure feature data are floats and labels are of long type for classification.\n",
    "X_train = torch.from_numpy(X_train_np).float().to(device)\n",
    "X_test = torch.from_numpy(X_test_np).float().to(device)\n",
    "y_train = torch.from_numpy(y_train_np).long().to(device)\n",
    "y_test = torch.from_numpy(y_test_np).long().to(device)\n",
    "\n",
    "# Create a Dataset and DataLoader for the training data\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the MLP model using PyTorch's nn.Module\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_units, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        # First hidden layer\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_units)\n",
    "        # Second hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_units, hidden_units)\n",
    "        # Output layer\n",
    "        self.out = nn.Linear(hidden_units, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply first layer and activation\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        # Second layer and activation\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        # Note: Do not apply softmax here when using CrossEntropyLoss.\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "# Model hyperparameters\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_units = 256\n",
    "output_dim = 10  # adjust based on your problem's number of classes\n",
    "\n",
    "# Instantiate and move the model to the GPU\n",
    "model = MLP(input_dim, hidden_units, output_dim).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * batch_X.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Evaluation on the test data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    # Get the predicted classes from the logits\n",
    "    _, y_pred_tensor = torch.max(test_outputs, 1)\n",
    "    # Bring tensors back to cpu and convert to numpy arrays for scikit-learn metrics\n",
    "    y_pred = y_pred_tensor.cpu().numpy()\n",
    "    y_test_np = y_test.cpu().numpy()\n",
    "    \n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test_np, y_pred)\n",
    "precision = precision_score(y_test_np, y_pred, average='macro')\n",
    "recall = recall_score(y_test_np, y_pred, average='macro')\n",
    "f1 = f1_score(y_test_np, y_pred, average='macro')\n",
    "\n",
    "print(f\"\\nEvaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"------------------------\")\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test_np, y_pred)\n",
    "\n",
    "# Calculate false alarms for each class \n",
    "# (sum of column values except the diagonal elements)\n",
    "false_alarms = cm.sum(axis=0) - np.diag(cm)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"False Alarms for each class:\", false_alarms)\n",
    "print(\"False Alarms for all classes:\", false_alarms.sum())\n",
    "\n",
    "# Save the scripted model\n",
    "model = torch.jit.script(model)\n",
    "torch.jit.save(model, 'Models and Data splits/MLP2L.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e4db64-5832-4aea-a231-7819f54abada",
   "metadata": {},
   "source": [
    "### CNN with 2 dense layer, one 128 and the other 64 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04862528-f63b-4dcb-a889-e6cb93dc6273",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/100], Loss: 0.2815\n",
      "Epoch [10/100], Loss: 0.0086\n",
      "Epoch [20/100], Loss: 0.0042\n",
      "Epoch [30/100], Loss: 0.0011\n",
      "Epoch [40/100], Loss: 0.0000\n",
      "Epoch [50/100], Loss: 0.0000\n",
      "Epoch [60/100], Loss: 0.0000\n",
      "Epoch [70/100], Loss: 0.0028\n",
      "Epoch [80/100], Loss: 0.0029\n",
      "Epoch [90/100], Loss: 0.0000\n",
      "Epoch [100/100], Loss: 0.0000\n",
      "\n",
      "Evaluation Metrics:\n",
      "Accuracy: 0.9878\n",
      "Precision: 0.9877\n",
      "Recall: 0.9877\n",
      "F1 Score: 0.9877\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1376    1    0    0    0    1    1    0    2    0]\n",
      " [   0 1565    3    0    1    0    1    4    1    0]\n",
      " [   3    0 1376    4    1    0    2    8    4    0]\n",
      " [   1    0    3 1408    0    8    1    2    3    2]\n",
      " [   1    1    0    0 1343    0    5    1    0   14]\n",
      " [   1    0    0    6    0 1250    2    1    2    1]\n",
      " [   7    2    0    0    3    2 1359    0    2    0]\n",
      " [   1    2    3    1    2    0    0 1445    2    3]\n",
      " [   2    2    2    3    0    3    3    1 1345    4]\n",
      " [   0    3    1    1    8    6    1    7    2 1362]]\n",
      "\n",
      "False Alarms for each class: [16 11 12 15 15 20 16 24 18 24]\n",
      "False Alarms for all classes: 171\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "import random  # Make sure to import random\n",
    "\n",
    "# -------------------------\n",
    "# Set seeds for reproducibility\n",
    "# -------------------------\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 1. Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Load data (X_train, X_test, y_train, y_test) from pickle\n",
    "X_train_np, X_test_np, y_train_np, y_test_np = joblib.load( 'Models and Data splits/[SCALED] Train_Test_Splits.pkl')\n",
    "\n",
    "\n",
    "# 3. Reshape data for PyTorch CNN: from (N, 28, 28, 1) to (N, 1, 28, 28)\n",
    "N_train = X_train_np.shape[0]\n",
    "N_test = X_test_np.shape[0]\n",
    "X_train_np = X_train_np.reshape(N_train, 1, 28, 28)\n",
    "X_test_np = X_test_np.reshape(N_test, 1, 28, 28)\n",
    "\n",
    "# 4. Convert numpy arrays to torch tensors, move them to device\n",
    "X_train = torch.from_numpy(X_train_np).float().to(device)\n",
    "y_train = torch.from_numpy(y_train_np).long().to(device)\n",
    "X_test = torch.from_numpy(X_test_np).float().to(device)\n",
    "y_test = torch.from_numpy(y_test_np).long().to(device)\n",
    "\n",
    "# 5. Create DataLoader for the training data\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 6. Define the CNN model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=0)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(32 * 13 * 13, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = CNNModel().to(device)\n",
    "\n",
    "# 7. Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 8. Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * batch_X.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# 9. Evaluation on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs_test = model(X_test)\n",
    "    _, predicted = torch.max(outputs_test, 1)\n",
    "\n",
    "y_pred = predicted.cpu().numpy()\n",
    "y_true = y_test.cpu().numpy()\n",
    "\n",
    "# 10. Evaluation metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='macro')\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# 11. Compute confusion matrix and false alarms\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "false_alarms = cm.sum(axis=0) - np.diag(cm)\n",
    "print(\"\\nFalse Alarms for each class:\", false_alarms)\n",
    "print(\"False Alarms for all classes:\", false_alarms.sum())\n",
    "\n",
    "# Save the scripted model\n",
    "model = torch.jit.script(model)\n",
    "torch.jit.save(model, 'Models and Data splits/CNN.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba87858-a7e9-49ff-8c5e-048be3e93ab4",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c65a50c2-b9bd-447b-825d-97720e010aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9696\n",
      "Precision: 0.9701\n",
      "Recall: 0.9692\n",
      "F1 Score: 0.9695\n",
      "--------------------\n",
      "Confusion Matrix:\n",
      " [[1375    1    0    0    0    1    3    0    0    1]\n",
      " [   0 1564    5    1    1    0    1    3    0    0]\n",
      " [  12   14 1335    4    2    0    4   24    3    0]\n",
      " [   1    2    7 1386    0   12    2    9    6    3]\n",
      " [   2   15    1    0 1306    0    5    1    0   35]\n",
      " [   6    3    1    7    1 1217   17    2    3    6]\n",
      " [   7    3    0    0    1    8 1355    0    1    0]\n",
      " [   1   17    3    0    2    1    0 1423    0   12]\n",
      " [   8   12    1   22    4   29    4    4 1275    6]\n",
      " [   2    8    1   13   11    1    1   13    2 1339]]\n",
      "False alarms for each class: [39 75 19 47 22 52 37 56 15 63]\n",
      "False alarms for all classes: 425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Models and Data splits/kNN.pkl']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Loading the pre-split data for reference \n",
    "X_train, X_test, y_train, y_test = joblib.load( 'Models and Data splits/[SCALED] Train_Test_Splits.pkl')\n",
    "\n",
    "# Define the kNN model\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')  # 'macro' averages for multi-class classification\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"--------------------\")\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# False alarms for each class (sum of columns except the diagonal)\n",
    "false_alarms = cm.sum(axis=0) - np.diagonal(cm)\n",
    "\n",
    "# Print false alarms for each class\n",
    "print(\"False alarms for each class:\", false_alarms)\n",
    "print(\"False alarms for all classes:\", false_alarms.sum())\n",
    "\n",
    "#Saving the model \n",
    "joblib.dump(model, 'Models and Data splits/kNN.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7a4315-8a61-47d7-863d-b20c9cfffeb4",
   "metadata": {},
   "source": [
    "## Ensemble Method: RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5289bb56-5a1e-44ee-8bf7-baf3a0e5470b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9672\n",
      "Precision: 0.9670\n",
      "Recall: 0.9669\n",
      "F1 Score: 0.9669\n",
      "-----------------------\n",
      "Confusion Matrix:\n",
      " [[1371    1    0    0    1    0    2    0    5    1]\n",
      " [   0 1548    9    7    1    1    2    5    1    1]\n",
      " [   8    1 1350    3    6    1    7   14    7    1]\n",
      " [   3    1   14 1368    0   14    2   12   12    2]\n",
      " [   4    3    1    0 1312    0   10    0    4   31]\n",
      " [   5    1    2   17    3 1207   11    2    7    8]\n",
      " [   9    2    1    0    3    4 1350    0    6    0]\n",
      " [   1    5   13    0    9    0    0 1414    3   14]\n",
      " [   3    3    8    7    3   11    3    0 1308   19]\n",
      " [   8    7    1   16   18    4    1   12   11 1313]]\n",
      "False Alarms for each class: [41 24 49 50 44 35 38 45 56 77]\n",
      "False Alarms for all classes: 459\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Models and Data splits/RF.pkl']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    " \n",
    "# Loading the pre-split data for reference \n",
    "X_train, X_test, y_train, y_test = joblib.load( 'Models and Data splits/[SCALED] Train_Test_Splits.pkl')\n",
    "\n",
    "\n",
    "# Define the Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')  # 'macro' averages for multi-class classification\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"-----------------------\")\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# False alarms for each class (sum of columns except the diagonal)\n",
    "false_alarms = cm.sum(axis=0) - np.diagonal(cm)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Print false alarms for each class\n",
    "print(\"False Alarms for each class:\", false_alarms)\n",
    "\n",
    "print(\"False Alarms for all classes:\", false_alarms.sum())\n",
    "\n",
    "#Saving the train and test sets \n",
    "joblib.dump(model, 'Models and Data splits/RF.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af188943-1e78-44c3-9ef1-574d09871d49",
   "metadata": {},
   "source": [
    "# MLP1L with StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "83bf14e2-a8ec-48c4-8063-6b5d52fabe7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.2215\n",
      "Epoch [2/100], Loss: 0.0935\n",
      "Epoch [3/100], Loss: 0.0604\n",
      "Epoch [4/100], Loss: 0.0411\n",
      "Epoch [5/100], Loss: 0.0294\n",
      "Epoch [6/100], Loss: 0.0264\n",
      "Epoch [7/100], Loss: 0.0195\n",
      "Epoch [8/100], Loss: 0.0187\n",
      "Epoch [9/100], Loss: 0.0178\n",
      "Epoch [10/100], Loss: 0.0307\n",
      "Epoch [11/100], Loss: 0.0157\n",
      "Epoch [12/100], Loss: 0.0116\n",
      "Epoch [13/100], Loss: 0.0061\n",
      "Epoch [14/100], Loss: 0.0074\n",
      "Epoch [15/100], Loss: 0.0314\n",
      "Epoch [16/100], Loss: 0.0185\n",
      "Epoch [17/100], Loss: 0.0071\n",
      "Epoch [18/100], Loss: 0.0069\n",
      "Epoch [19/100], Loss: 0.0104\n",
      "Epoch [20/100], Loss: 0.0163\n",
      "Epoch [21/100], Loss: 0.0091\n",
      "Epoch [22/100], Loss: 0.0074\n",
      "Epoch [23/100], Loss: 0.0257\n",
      "Epoch [24/100], Loss: 0.0180\n",
      "Epoch [25/100], Loss: 0.0294\n",
      "Epoch [26/100], Loss: 0.0148\n",
      "Epoch [27/100], Loss: 0.0126\n",
      "Epoch [28/100], Loss: 0.0067\n",
      "Epoch [29/100], Loss: 0.0055\n",
      "Epoch [30/100], Loss: 0.0090\n",
      "Epoch [31/100], Loss: 0.0096\n",
      "Epoch [32/100], Loss: 0.0161\n",
      "Epoch [33/100], Loss: 0.0170\n",
      "Epoch [34/100], Loss: 0.0100\n",
      "Epoch [35/100], Loss: 0.0132\n",
      "Epoch [36/100], Loss: 0.0081\n",
      "Epoch [37/100], Loss: 0.0041\n",
      "Epoch [38/100], Loss: 0.0095\n",
      "Epoch [39/100], Loss: 0.0104\n",
      "Epoch [40/100], Loss: 0.0049\n",
      "Epoch [41/100], Loss: 0.0150\n",
      "Epoch [42/100], Loss: 0.0336\n",
      "Epoch [43/100], Loss: 0.0135\n",
      "Epoch [44/100], Loss: 0.0056\n",
      "Epoch [45/100], Loss: 0.0117\n",
      "Epoch [46/100], Loss: 0.0145\n",
      "Epoch [47/100], Loss: 0.0103\n",
      "Epoch [48/100], Loss: 0.0071\n",
      "Epoch [49/100], Loss: 0.0101\n",
      "Epoch [50/100], Loss: 0.0080\n",
      "Epoch [51/100], Loss: 0.0070\n",
      "Epoch [52/100], Loss: 0.0046\n",
      "Epoch [53/100], Loss: 0.0212\n",
      "Epoch [54/100], Loss: 0.0174\n",
      "Epoch [55/100], Loss: 0.0090\n",
      "Epoch [56/100], Loss: 0.0097\n",
      "Epoch [57/100], Loss: 0.0197\n",
      "Epoch [58/100], Loss: 0.0034\n",
      "Epoch [59/100], Loss: 0.0247\n",
      "Epoch [60/100], Loss: 0.0201\n",
      "Epoch [61/100], Loss: 0.0054\n",
      "Epoch [62/100], Loss: 0.0021\n",
      "Epoch [63/100], Loss: 0.0092\n",
      "Epoch [64/100], Loss: 0.0090\n",
      "Epoch [65/100], Loss: 0.0074\n",
      "Epoch [66/100], Loss: 0.0379\n",
      "Epoch [67/100], Loss: 0.0367\n",
      "Epoch [68/100], Loss: 0.0073\n",
      "Epoch [69/100], Loss: 0.0052\n",
      "Epoch [70/100], Loss: 0.0072\n",
      "Epoch [71/100], Loss: 0.0026\n",
      "Epoch [72/100], Loss: 0.0055\n",
      "Epoch [73/100], Loss: 0.0060\n",
      "Epoch [74/100], Loss: 0.0438\n",
      "Epoch [75/100], Loss: 0.0180\n",
      "Epoch [76/100], Loss: 0.0072\n",
      "Epoch [77/100], Loss: 0.0037\n",
      "Epoch [78/100], Loss: 0.0013\n",
      "Epoch [79/100], Loss: 0.0024\n",
      "Epoch [80/100], Loss: 0.0088\n",
      "Epoch [81/100], Loss: 0.0139\n",
      "Epoch [82/100], Loss: 0.0074\n",
      "Epoch [83/100], Loss: 0.0064\n",
      "Epoch [84/100], Loss: 0.0215\n",
      "Epoch [85/100], Loss: 0.0052\n",
      "Epoch [86/100], Loss: 0.0488\n",
      "Epoch [87/100], Loss: 0.0647\n",
      "Epoch [88/100], Loss: 0.0097\n",
      "Epoch [89/100], Loss: 0.0024\n",
      "Epoch [90/100], Loss: 0.0045\n",
      "Epoch [91/100], Loss: 0.0107\n",
      "Epoch [92/100], Loss: 0.0112\n",
      "Epoch [93/100], Loss: 0.0016\n",
      "Epoch [94/100], Loss: 0.0888\n",
      "Epoch [95/100], Loss: 0.0137\n",
      "Epoch [96/100], Loss: 0.0759\n",
      "Epoch [97/100], Loss: 0.0292\n",
      "Epoch [98/100], Loss: 0.0177\n",
      "Epoch [99/100], Loss: 0.0007\n",
      "Epoch [100/100], Loss: 0.0019\n",
      "\n",
      "Accuracy: 0.9753\n",
      "Precision: 0.9752\n",
      "Recall: 0.9752\n",
      "F1 Score: 0.9751\n",
      "--------------\n",
      "Confusion Matrix:\n",
      " [[1375    1    0    1    1    1    0    0    1    1]\n",
      " [   0 1550    8    3    1    4    1    6    2    0]\n",
      " [   7    3 1357    7    1    2    4    5    7    5]\n",
      " [   1    1   12 1384    1   12    0   10    4    3]\n",
      " [   1    3    2    0 1325    0    6    2    2   24]\n",
      " [   3    1    0    6    3 1228    7    3    6    6]\n",
      " [   8    2    2    0    2    8 1350    0    3    0]\n",
      " [   3    3   11    2    7    2    0 1417    0   14]\n",
      " [   3    3    6    9    3   11    6    2 1313    9]\n",
      " [   2    4    0    3    7    4    0   15    1 1355]]\n",
      "False Alarms for each class: [28 21 41 31 26 44 24 43 26 62]\n",
      "False Alarms for all classes: 346\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import random\n",
    "import joblib\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# -------------------------\n",
    "# Set seeds for reproducibility\n",
    "# -------------------------\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    # Optional: for determinism (may impact performance)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# Loading the original MNIST Data\n",
    "# -------------------------\n",
    "X_train_np, X_test_np, y_train_np, y_test_np = joblib.load('Models and Data splits/[ORIGINAL] Train_Test_Splits.pkl')\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train_np)\n",
    "\n",
    "# Transform the test data using the same scaler\n",
    "X_test_scaled = scaler.transform(X_test_np)\n",
    "\n",
    "# Save the scaler to a file for later use\n",
    "joblib.dump(scaler, 'Models and Data splits/scaler.pkl')\n",
    "\n",
    "# Convert scaled data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_np, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test_np, dtype=torch.long)\n",
    "\n",
    "# Create DataLoaders for mini-batch training\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# -------------------------\n",
    "# Define the MLPSca Model using PyTorch\n",
    "# -------------------------\n",
    "class MLPSca(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_units, output_dim):\n",
    "        super(MLPSca, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_units)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_units, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        # The output logits will be used in the CrossEntropyLoss, so no softmax here\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MLPSca(input_dim=X_train.shape[1], hidden_units=256, output_dim=10)\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# -------------------------\n",
    "# Train the Model\n",
    "# -------------------------\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # Move data to GPU if available\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Evaluate the Model on the Test Data\n",
    "# -------------------------\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_true = []\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        # Get predicted class using the index with maximum logit score\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.append(predicted.cpu().numpy())\n",
    "        all_true.append(batch_y.cpu().numpy())\n",
    "\n",
    "y_pred = np.concatenate(all_preds)\n",
    "y_true = np.concatenate(all_true)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='macro')\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"--------------\")\n",
    "\n",
    "# Compute the confusion matrix and false positives per class\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "false_alarms = cm.sum(axis=0) - np.diag(cm)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"False Alarms for each class:\", false_alarms)\n",
    "print(\"False Alarms for all classes:\", false_alarms.sum())\n",
    "\n",
    "# Save the scripted model\n",
    "model = torch.jit.script(model)\n",
    "torch.jit.save(model, 'Models and Data splits/MainModel_MLPSca.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4644257-b7ee-444b-b5e1-e7ae153c27ae",
   "metadata": {},
   "source": [
    "### CNN with different Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a39dc17b-6ebf-4a5c-8986-cf46c79d772e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/100]  Loss: 0.2202\n",
      "Epoch [10/100]  Loss: 0.0130\n",
      "Epoch [20/100]  Loss: 0.0115\n",
      "Epoch [30/100]  Loss: 0.0043\n",
      "Epoch [40/100]  Loss: 0.0072\n",
      "Epoch [50/100]  Loss: 0.0025\n",
      "Epoch [60/100]  Loss: 0.0079\n",
      "Epoch [70/100]  Loss: 0.0076\n",
      "Epoch [80/100]  Loss: 0.0000\n",
      "Epoch [90/100]  Loss: 0.0000\n",
      "Epoch [100/100]  Loss: 0.0000\n",
      "\n",
      "Evaluation Metrics\n",
      "Accuracy : 0.9859\n",
      "Precision: 0.9859\n",
      "Recall   : 0.9858\n",
      "F1 Score : 0.9858\n",
      "\n",
      "Confusion Matrix\n",
      "[[1375    1    2    1    0    0    2    0    0    0]\n",
      " [   0 1558    4    2    1    1    2    7    0    0]\n",
      " [   1    2 1374    2    2    0    5    6    4    2]\n",
      " [   1    3    2 1411    0    4    0    4    3    0]\n",
      " [   0    3    1    0 1337    0    5    3    0   16]\n",
      " [   1    0    0    4    0 1243    5    1    3    6]\n",
      " [   7    2    0    0    2    3 1359    0    2    0]\n",
      " [   1    2    8    1    1    0    0 1432    2   12]\n",
      " [   4    2    5    4    1    3    2    1 1339    4]\n",
      " [   0    2    0    0    2    3    0    7    3 1374]]\n",
      "\n",
      "False Alarms per class: [15 17 22 14  9 14 21 29 17 40]\n",
      "Total False Alarms    : 198\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import random\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility\n",
    "# -------------------------\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# -------------------------\n",
    "# Device\n",
    "# -------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 1. Load ORIGINAL (unscaled) data\n",
    "# -----------------------------------------------------\n",
    "X_train_np, X_test_np, y_train_np, y_test_np = joblib.load(\n",
    "    \"Models and Data splits/[ORIGINAL] Train_Test_Splits.pkl\"\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 2. Standardscale pixel intensities\n",
    "#    (fit on TRAIN, transform TRAIN & TEST)\n",
    "# -----------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "\n",
    "N_train = X_train_np.shape[0]\n",
    "N_test  = X_test_np.shape[0]\n",
    "\n",
    "# Flatten  scale  reshape\n",
    "X_train_flat = X_train_np.reshape(N_train, -1).astype(np.float32)\n",
    "X_test_flat  = X_test_np.reshape(N_test,  -1).astype(np.float32)\n",
    "\n",
    "X_train_flat = scaler.fit_transform(X_train_flat)\n",
    "X_test_flat  = scaler.transform(X_test_flat)\n",
    "\n",
    "X_train_np = X_train_flat.reshape(N_train, 1, 28, 28)\n",
    "X_test_np  = X_test_flat.reshape(N_test,  1, 28, 28)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 3. Convert to tensors\n",
    "# -----------------------------------------------------\n",
    "X_train = torch.from_numpy(X_train_np).float().to(device)\n",
    "y_train = torch.from_numpy(y_train_np).long().to(device)\n",
    "X_test  = torch.from_numpy(X_test_np).float().to(device)\n",
    "y_test  = torch.from_numpy(y_test_np).long().to(device)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 4. DataLoader\n",
    "# -----------------------------------------------------\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader  = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 5. CNN definition\n",
    "# -----------------------------------------------------\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)   # (283+1)=26  pool  13\n",
    "        self.pool  = nn.MaxPool2d(2)\n",
    "        self.fc1   = nn.Linear(32 * 13 * 13, 128)\n",
    "        self.fc2   = nn.Linear(128, 64)\n",
    "        self.fc3   = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "model = CNNModel().to(device)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 6. Loss & optimiser\n",
    "# -----------------------------------------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 7. Training\n",
    "# -----------------------------------------------------\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * batch_X.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]  Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 8. Evaluation\n",
    "# -----------------------------------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs_test = model(X_test)\n",
    "    _, predicted = torch.max(outputs_test, 1)\n",
    "\n",
    "y_pred = predicted.cpu().numpy()\n",
    "y_true = y_test.cpu().numpy()\n",
    "\n",
    "accuracy  = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average=\"macro\")\n",
    "recall    = recall_score(y_true, y_pred, average=\"macro\")\n",
    "f1        = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "print(\"\\nEvaluation Metrics\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"\\nConfusion Matrix\")\n",
    "print(cm)\n",
    "\n",
    "false_alarms = cm.sum(axis=0) - np.diag(cm)\n",
    "print(\"\\nFalse Alarms per class:\", false_alarms)\n",
    "print(\"Total False Alarms    :\", false_alarms.sum())\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 9. Save the scripted model\n",
    "# -----------------------------------------------------\n",
    "scripted = torch.jit.script(model)\n",
    "torch.jit.save(scripted, \"Models and Data splits/CNNScl.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:GPUEnabled]",
   "language": "python",
   "name": "conda-env-GPUEnabled-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
